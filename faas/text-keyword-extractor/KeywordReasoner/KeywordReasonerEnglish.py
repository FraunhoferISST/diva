import nltk
from nltk.stem import PorterStemmer

# Stemmer for reducing words to base
ps = PorterStemmer()

# Params:
# keywords_scores_rake: Keywords generated by Multi-RAKE with score
# keywords_rake:        Keywords generated by Multi-RAKE without score
# text:                 Text used for keyword extraction

def KeywordReasonerEnglish(keywords_rake, text, maxResults):

    # stemm keywordlist for removing double entries of singular and plural
    keywords_stemmed_rake = list(map(lambda x: (x, ps.stem(x)), keywords_rake))

    tmpKeywords = []

    # iterate over all items of stemmed keywords
    for sk in keywords_stemmed_rake:

        exists = False

        for k in tmpKeywords:
            if k[1] == sk[1]:
                exists = True
                break

        if not exists:
            tmpKeywords.append(sk)

    keywords_rake = [x[0] for x in tmpKeywords]

    # stopwords
    default_stopwords = set(nltk.corpus.stopwords.words('english'))

    # words of text
    wordsText = nltk.tokenize.word_tokenize(text, language='english')

    # Remove single-character tokens (mostly punctuation)
    wordsText = [word for word in wordsText if len(word) > 1]

    # Remove numbers
    wordsText = [word for word in wordsText if not word.isnumeric()]

    # Lowercase all words (default_stopwords are lowercase too)
    wordsText = [word.lower() for word in wordsText]

    # Remove stopwords
    wordsText = [word for word in wordsText if word not in default_stopwords]

    # keyword frequency calculation
    freqWordsText = nltk.FreqDist(wordsText)

    # make tupel list of keywords for collection of identified word types
    keywords_types = list(map(lambda x: (x, []), keywords_rake))  # [ ('erde', []), ... ]

    # tokenize text into sentences
    # sentences are needed for the NLP Machine Learned POS Tagger
    sentences = nltk.tokenize.sent_tokenize(text, language='english')

    # iterate over every sentence (there are statisticaly more sentences then keywords)
    for sent in sentences:

        # tokenize sentence into words
        words = nltk.tokenize.word_tokenize(sent, language='english')

        # words to lower string
        wordsLower = list(map(lambda x: x.lower(), words))

        # check if
        matches = [k for k in keywords_rake if k in wordsLower]

        # if there is a match, we need to proceed
        if matches:

            # tagging words, if sentence contains one or more keywords
            wordsTagged = nltk.pos_tag(words)

            for wt in wordsTagged:
                for kt in keywords_types:
                    if wt[0].lower() == kt[0]:
                        kt[1].append(wt[1])

    keywords = []

    for kt in keywords_types:
        if kt[1]:
            fdist = nltk.FreqDist(kt[1])  # creates a frequency distribution from a list
            most_common = fdist.max()
            if most_common == 'NN' or most_common == 'NNS' or most_common == 'NNP':
                keywords.append(kt[0].title())

    result = []

    for k in keywords:
        result.append((k, freqWordsText[k.lower()]))

    result = sorted(result, key=lambda tup: tup[1], reverse=True)
    result = list(map(lambda x: x[0], result))[:maxResults]

    return result