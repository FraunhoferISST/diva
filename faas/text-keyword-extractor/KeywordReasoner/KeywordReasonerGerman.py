import pickle
import os
import nltk
import re

from ClassifierBasedGermanTagger.ClassifierBasedGermanTagger import ClassifierBasedGermanTagger

stripge = re.compile(r"^ge(.{4,})")
replxx = re.compile(r"(.)\1")
replxxback = re.compile(r"(.)\*")
stripemr = re.compile(r"e[mr]$")
stripnd = re.compile(r"nd$")
stript = re.compile(r"t$")
stripesn = re.compile(r"[esn]$")


# Params:
# keywords_scores_rake: Keywords generated by Multi-RAKE with score
# keywords_rake:        Keywords generated by Multi-RAKE without score
# text:                 Text used for keyword extraction

def KeywordReasonerGerman(keywords_rake, text, maxResults):

    # load german pos (part of speech) tagger
    fn = os.path.join(os.path.dirname(__file__), 'nltk_german_classifier_data.pickle')
    with open(fn, 'rb') as f:
        taggerGerman = pickle.load(f)

    # stemm keywordlist for removing double entries of singular and plural
    keywords_stemmed_rake = list(map(lambda x: (x, stem(x)), keywords_rake))

    tmpKeywords = []

    # iterate over all items of stemmed keywords
    for sk in keywords_stemmed_rake:

        exists = False

        for k in tmpKeywords:
            if k[1] == sk[1]:
                exists = True
                break

        if not exists:
            tmpKeywords.append(sk)

    keywords_rake = [x[0] for x in tmpKeywords]


    # stopwords
    default_stopwords = set(nltk.corpus.stopwords.words('german'))

    # words of text
    wordsText = nltk.tokenize.word_tokenize(text, language='german')

    # Remove single-character tokens (mostly punctuation)
    wordsText = [word for word in wordsText if len(word) > 1]

    # Remove numbers
    wordsText = [word for word in wordsText if not word.isnumeric()]

    # Lowercase all words (default_stopwords are lowercase too)
    wordsText = [word.lower() for word in wordsText]

    # Remove stopwords
    wordsText = [word for word in wordsText if word not in default_stopwords]

    # keyword frequency calculation
    freqWordsText = nltk.FreqDist(wordsText)

    # make tupel list of keywords for collection of identified word types
    keywords_types = list(map(lambda x: (x, []), keywords_rake))  # [ ('erde', []), ... ]

    # tokenize text into sentences
    # sentences are needed for the NLP Machine Learned POS Tagger
    sentences = nltk.tokenize.sent_tokenize(text, language='german')

    # iterate over every sentence (there are statisticaly more sentences then keywords)
    for sent in sentences:

        # tokenize sentence into words
        words = nltk.tokenize.word_tokenize(sent, language='german')

        # words to lower string
        wordsLower = list(map(lambda x: x.lower(), words))

        # check if
        matches = [k for k in keywords_rake if k in wordsLower]

        # if there is a match, we need to proceed
        if matches:

            # tagging words, if sentence contains one or more keywords
            wordsTagged = taggerGerman.tag(words)

            for wt in wordsTagged:
                for kt in keywords_types:
                    if wt[0].lower() == kt[0]:
                        kt[1].append(wt[1])

    keywords = []

    for kt in keywords_types:
        if kt[1]:
            fdist = nltk.FreqDist(kt[1])  # creates a frequency distribution from a list
            most_common = fdist.max()
            if most_common == 'NN' or most_common == 'NE':
                keywords.append(kt[0].title())

    result = []

    for k in keywords:
        result.append((k, freqWordsText[k.lower()]))

    result = sorted(result, key=lambda tup: tup[1], reverse=True)
    result = list(map(lambda x: x[0], result))[:maxResults]

    return result


"""
This method takes the word to be stemmed and a boolean specifiying if case-insensitive stemming should be used and returns the stemmed word. If only the word
is passed to the method or the second parameter is 0, normal case-sensitive stemming is used, if the second parameter is 1, case-insensitive stemming is used.
Case sensitivity improves performance only if words in the text may be incorrectly upper case.
For all-lowercase and correctly cased text, best performance is achieved by
using the case-sensitive version.
"""

def stem(word, case_insensitive = False):
    if len(word) == 0:
        return word

    upper = word[0].isupper()
    word = word.lower()

    word = word.replace("ü","u")
    word = word.replace("ö","o")
    word = word.replace("ä","a")
    word = word.replace("ß","ss")

    word = stripge.sub(r"\1", word)
    word = word.replace("sch","$")
    word = word.replace("ei","%")
    word = word.replace("ie","&")
    word = replxx.sub(r"\1*", word)

    while len(word) > 3:
        if len(word) > 5:
            (word, success) = stripemr.subn("", word)
            if success != 0:
                continue

            (word, success) = stripnd.subn("", word)
            if success != 0:
                continue

        if not upper or case_insensitive:
            (word, success) = stript.subn("", word)
            if success != 0:
                continue

        (word, success) = stripesn.subn("", word)
        if success != 0:
            continue
        else:
            break

    word = replxxback.sub(r"\1\1", word)
    word = word.replace("%","ei")
    word = word.replace("&","ie")
    word = word.replace("$","sch")

    return word
